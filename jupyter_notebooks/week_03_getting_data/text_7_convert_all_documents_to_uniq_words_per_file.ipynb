{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `sent_tokenize` here, but I'll use simple split on space approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = pickle.load( open( \"all_documents.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['essays/week1_1.txt', 'essays/week1_380.txt', 'essays/week1_summary.docx', 'essays/week2_Summary Week 2.docx', 'essays/week1_Data Wrangling Chap 2.docx', 'essays/week2_summary-Data Wrangling with Python  ch2 p17 to 40.docx', 'essays/week2_Week 2 - Summary of DATA WRANGLING WITH PYTHON Chapter 2.docx', 'essays/week2_Data Wrangling with Python page 17 to 40.docx', 'essays/week1_a Very Short History Of Data Science_1.docx', 'essays/week2_Lists and Dictionaries Summary.docx', 'essays/week2_Data601-Reading Assignment_2.docx', 'essays/week1_a History of Data Science.docx', 'essays/week1_Assignment1.docx', 'essays/week1_reading Summary.docx', 'essays/week1_A Very Short History Of Data Science.docx', 'essays/week1_Data 601- Summary of The History of Data Science .docx', 'essays/week1_50 Years Data Science Summary.docx', 'essays/week2_Summary on Chapter 2 of Data Wrangling in Python.docx', 'essays/week2_Week 2 Reading Summary.docx', 'essays/Week2_summary.docx', 'essays/week1_summary-50 years of data science.docx', 'essays/week1_ Summary.docx', 'essays/week2 summary .docx', 'essays/week1_50 years of data science v2.pdf', 'essays/week1_assignment 1 Summary.pdf'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply the same clean-up steps taken in the previous notebook, but this time per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path,content in all_documents.items():\n",
    "    # make entire string lowercase\n",
    "    single_string_all_lowercase = content.lower()\n",
    "    # split string into words based on space\n",
    "    list_of_words=single_string_all_lowercase.split(\" \")\n",
    "    # remove punctuation; alternatively we could have used regular expressions\n",
    "    # an expanded version of \n",
    "    # list_of_words = [''.join(c for c in s if c not in string.punctuation) for s in list_of_words]\n",
    "    list_of_words_without_punctuation=[]\n",
    "    for this_string in list_of_words:\n",
    "        new_list_of_char=[]\n",
    "        for this_char in this_string:\n",
    "            if (this_char in string.ascii_lowercase):\n",
    "                new_list_of_char.append(this_char)\n",
    "        new_string=''.join(new_list_of_char)\n",
    "        list_of_words_without_punctuation.append(new_string)\n",
    "    # remove empty strings\n",
    "    list_of_words_without_punctuation = list(filter(None, list_of_words_without_punctuation))\n",
    "    # remove stop words\n",
    "    filtered_word_list = [] \n",
    "    for w in list_of_words_without_punctuation: \n",
    "        if w not in en_stops: \n",
    "            filtered_word_list.append(w) \n",
    "    # remove duplicates\n",
    "    filtered_word_list_uniq = list(set(filtered_word_list))\n",
    "    all_documents[file_path]=filtered_word_list_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path,content in all_documents.items():\n",
    "    with open(file_path+\".dat\",'w') as fdoc:\n",
    "        for this_word in content:\n",
    "            fdoc.write(this_word+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " file_text.txt\n",
      " week1_1.txt\n",
      " week1_1.txt.dat\n",
      " week1_380.txt\n",
      " week1_380.txt.dat\n",
      "'week1_50 Years Data Science Summary.docx'\n",
      "'week1_50 Years Data Science Summary.docx.dat'\n",
      "'week1_50 years of data science v2.pdf'\n",
      "'week1_50 years of data science v2.pdf.dat'\n",
      "'week1_a History of Data Science.docx'\n",
      "'week1_a History of Data Science.docx.dat'\n",
      " week1_Assignment1.docx\n",
      " week1_Assignment1.docx.dat\n",
      "'week1_assignment 1 Summary.pdf'\n",
      "'week1_assignment 1 Summary.pdf.dat'\n",
      "'week1_a Very Short History Of Data Science_1.docx'\n",
      "'week1_a Very Short History Of Data Science_1.docx.dat'\n",
      "'week1_A Very Short History Of Data Science.docx'\n",
      "'week1_A Very Short History Of Data Science.docx.dat'\n",
      "'week1_Data 601- Summary of The History of Data Science .docx'\n",
      "'week1_Data 601- Summary of The History of Data Science .docx.dat'\n",
      "'week1_Data Wrangling Chap 2.docx'\n",
      "'week1_Data Wrangling Chap 2.docx.dat'\n",
      "'week1_essay_2019-08-30-18-51-59_History of Data Science.docx'\n",
      " week1_essay_2019-08-30-18-51-59.txt\n",
      "'week1_essay_2019-08-30-21-06-35_A very short history on data science.docx'\n",
      " week1_essay_2019-08-30-21-06-35.txt\n",
      " week1_essay_2019-08-31-00-12-38.txt\n",
      " week1_essay_2019-09-01-14-15-13.txt\n",
      " week1_essay_2019-09-02-23-00-00.txt\n",
      "'week1_essay_2019-09-02-23-00-00_Week 1 Essay.txt'\n",
      " week1_essay_2019-09-02-23-40-03.txt\n",
      "'week1_essay_2019-09-03-09-09-52_Summary of 50 years of Data Science.txt'\n",
      " week1_essay_2019-09-03-09-09-52.txt\n",
      "'week1_essay_2019-09-03-10-08-00_assignment 1 short history.txt'\n",
      " week1_essay_2019-09-03-10-08-00.txt\n",
      " week1_essay_2019-09-03-11-08-21.txt\n",
      " week1_essay_2019-09-03-12-34-27.txt\n",
      " week1_essay_2019-09-03-13-14-27.txt\n",
      " week1_essay_2019-09-03-14-04-16.txt\n",
      " week1_essay_2019-09-03-14-14-24.txt\n",
      " week1_essay_2019-09-03-14-49-33.txt\n",
      " week1_essay_2019-09-03-14-50-26.txt\n",
      " week1_essay_2019-09-03-15-04-43.txt\n",
      " week1_essay_2019-09-03-15-30-55.txt\n",
      " week1_essay_2019-09-03-15-41-31.txt\n",
      " week1_essay_2019-09-03-15-47-41.txt\n",
      " week1_essay_2019-09-03-16-16-04.txt\n",
      " week1_essay_2019-09-03-16-19-10.txt\n",
      " week1_essay_2019-09-03-16-28-05.txt\n",
      "'week1_essay_2019-09-03-17-43-38_50 years of Data Science.docx'\n",
      " week1_essay_2019-09-03-17-43-38.txt\n",
      " week1_essay_2019-09-03-17-47-19.txt\n",
      " week1_essay.zip\n",
      "'week1_reading Summary.docx'\n",
      "'week1_reading Summary.docx.dat'\n",
      "'week1_summary-50 years of data science.docx'\n",
      "'week1_summary-50 years of data science.docx.dat'\n",
      " week1_summary.docx\n",
      "'week1_ Summary.docx'\n",
      " week1_summary.docx.dat\n",
      "'week1_ Summary.docx.dat'\n",
      "'week2_Data601-Reading Assignment_2.docx'\n",
      "'week2_Data601-Reading Assignment_2.docx.dat'\n",
      "'week2_Data Wrangling with Python page 17 to 40.docx'\n",
      "'week2_Data Wrangling with Python page 17 to 40.docx.dat'\n",
      "'week2_Lists and Dictionaries Summary.docx'\n",
      "'week2_Lists and Dictionaries Summary.docx.dat'\n",
      "'week2_summary-Data Wrangling with Python  ch2 p17 to 40.docx'\n",
      "'week2_summary-Data Wrangling with Python  ch2 p17 to 40.docx.dat'\n",
      "'week2 summary .docx'\n",
      " Week2_summary.docx\n",
      "'week2 summary .docx.dat'\n",
      " Week2_summary.docx.dat\n",
      "'week2_Summary on Chapter 2 of Data Wrangling in Python.docx'\n",
      "'week2_Summary on Chapter 2 of Data Wrangling in Python.docx.dat'\n",
      "'week2_Summary Week 2.docx'\n",
      "'week2_Summary Week 2.docx.dat'\n",
      "'week2_Week 2 Reading Summary.docx'\n",
      "'week2_Week 2 Reading Summary.docx.dat'\n",
      "'week2_Week 2 - Summary of DATA WRANGLING WITH PYTHON Chapter 2.docx'\n",
      "'week2_Week 2 - Summary of DATA WRANGLING WITH PYTHON Chapter 2.docx.dat'\n"
     ]
    }
   ],
   "source": [
    "!ls essays/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value\n",
      "covered\n",
      "week\n",
      "expectations\n",
      "journals\n",
      "especially\n",
      "created\n",
      "consensus\n",
      "data\n",
      "development\n",
      "media\n",
      "attempts\n",
      "statistical\n",
      "societies\n",
      "desires\n",
      "proposed\n",
      "discussion\n",
      "define\n",
      "like\n",
      "computer\n",
      "time\n",
      "amounts\n",
      "academic\n",
      "new\n",
      "moved\n",
      "found\n",
      "started\n",
      "aid\n",
      "early\n",
      "things\n",
      "happening\n",
      "format\n",
      "variety\n",
      "arena\n",
      "blogs\n",
      "sources\n",
      "imprecise\n",
      "associations\n",
      "mathematical\n",
      "conversation\n",
      "analysis\n",
      "esoteric\n",
      "computing\n",
      "circles\n",
      "many\n",
      "concrete\n",
      "large\n",
      "mathematicalstatistical\n",
      "possible\n",
      "interesting\n",
      "definitions\n",
      "history\n",
      "people\n",
      "developed\n",
      "seems\n",
      "useful\n",
      "science\n",
      "points\n",
      "years\n",
      "source\n",
      "summary\n",
      "popular\n",
      "present\n",
      "made\n",
      "involves\n",
      "matured\n",
      "similar\n",
      "industry\n",
      "specific\n",
      "short\n",
      "grew\n",
      "timeline\n",
      "accomplish\n",
      "article\n",
      "one\n",
      "beginning\n",
      "extract\n",
      "including\n",
      "information\n",
      "number\n",
      "opinion\n",
      "academia\n",
      "discipline\n",
      "suggested\n",
      "business\n",
      "norms\n",
      "became\n",
      "notion\n",
      "field\n",
      "changed\n",
      "notes\n",
      "giving\n",
      "subdiscipline\n",
      "nebulous\n",
      "overview\n",
      "evolved\n"
     ]
    }
   ],
   "source": [
    "!cat essays/week1_a\\ Very\\ Short\\ History\\ Of\\ Data\\ Science_1.docx.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
