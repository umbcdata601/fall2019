{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import fnmatch # https://docs.python.org/3/library/fnmatch.html\n",
    "import glob\n",
    "import doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://stevenloria.com/tf-idf/ <BR>\n",
    "\n",
    "Caveat: this post now uses TextBlob for breaking up the text into words and getting the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, list_of_words_in_document):\n",
    "    \"\"\"\n",
    "    computes \"term frequency\" which is the number of times a word appears in a document, \n",
    "    normalized by dividing by the total number of words in document. \n",
    "    \n",
    "    >>> term_freq('asdf',['asdf','lmka','mignasf'])\n",
    "    0.3333333333333333\n",
    "    \"\"\"\n",
    "    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_documents_containing(term,all_documents):\n",
    "    \"\"\"\n",
    "    Returns the number of documents containing word. \n",
    "    \n",
    "    >>> all_doc = [['asdf','ionasd'],['igag'],['ngi','adfmig','mgiaf']]\n",
    "    >>> number_of_documents_containing('asdf',all_doc)\n",
    "    1\n",
    "    \"\"\"\n",
    "    countr=0\n",
    "    for this_doc in all_documents:\n",
    "        if (term in this_doc):\n",
    "            countr+=1\n",
    "    return countr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_doc_freq(term, all_documents):\n",
    "    \"\"\"\n",
    "    computes \"inverse document frequency\" which measures \n",
    "    how common a word is among all documents in corpus. \n",
    "    The more common a word is, the lower its idf. \n",
    "    Take the ratio of the total number of documents to\n",
    "    the number of documents containing word, \n",
    "    then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "    \n",
    "    >>> all_doc = [['asdf','ionasd'],['igag'],['ngi','adfmig','mgiaf']]\n",
    "    >>> number_of_documents_containing('asdf',all_doc)\n",
    "    1\n",
    "    \"\"\"\n",
    "    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, list_of_words_in_document, all_documents):\n",
    "    \"\"\"\n",
    "    computes the TF-IDF score. It's the product of tf and idf.\n",
    "    \n",
    "    >>> all_doc = [['asdf','ionasd'],['igag']]\n",
    "    >>> tfidf('asdf',all_doc[0],all_doc)\n",
    "    0.0\n",
    "    \n",
    "    >>> all_doc = [['asdf','ionasd'],['igag','geaag'],['ngi','adfmig','mgiaf']]\n",
    "    >>> tfidf('igag',all_doc[1],all_doc)\n",
    "    0.2027325540540822\n",
    "    \"\"\"\n",
    "    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\*.dat files in the directory have only key words from each file\n",
    "\n",
    "Convert the .dat contents to lists per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 words in essays_word_per_file/week2 summary .docx.dat\n",
      "141 words in essays_word_per_file/week1_reading Summary.docx.dat\n",
      "140 words in essays_word_per_file/week1_Data 601- Summary of The History of Data Science .docx.dat\n",
      "78 words in essays_word_per_file/week2_summary-Data Wrangling with Python  ch2 p17 to 40.docx.dat\n",
      "100 words in essays_word_per_file/week1_a History of Data Science.docx.dat\n",
      "80 words in essays_word_per_file/week1_Data Wrangling Chap 2.docx.dat\n",
      "96 words in essays_word_per_file/week1_a Very Short History Of Data Science_1.docx.dat\n",
      "115 words in essays_word_per_file/week1_ Summary.docx.dat\n",
      "140 words in essays_word_per_file/week1_summary.docx.dat\n",
      "49 words in essays_word_per_file/week1_1.txt.dat\n",
      "119 words in essays_word_per_file/week2_Lists and Dictionaries Summary.docx.dat\n",
      "130 words in essays_word_per_file/week2_Data Wrangling with Python page 17 to 40.docx.dat\n",
      "81 words in essays_word_per_file/week2_Week 2 Reading Summary.docx.dat\n",
      "109 words in essays_word_per_file/Week2_summary.docx.dat\n",
      "117 words in essays_word_per_file/week2_Summary Week 2.docx.dat\n",
      "66 words in essays_word_per_file/week1_summary-50 years of data science.docx.dat\n",
      "119 words in essays_word_per_file/week2_Data601-Reading Assignment_2.docx.dat\n",
      "18 words in essays_word_per_file/week1_assignment 1 Summary.pdf.dat\n",
      "105 words in essays_word_per_file/week2_Summary on Chapter 2 of Data Wrangling in Python.docx.dat\n",
      "122 words in essays_word_per_file/week2_Week 2 - Summary of DATA WRANGLING WITH PYTHON Chapter 2.docx.dat\n",
      "106 words in essays_word_per_file/week1_380.txt.dat\n",
      "145 words in essays_word_per_file/week1_Assignment1.docx.dat\n",
      "94 words in essays_word_per_file/week1_A Very Short History Of Data Science.docx.dat\n",
      "133 words in essays_word_per_file/week1_50 Years Data Science Summary.docx.dat\n"
     ]
    }
   ],
   "source": [
    "all_documents={}\n",
    "all_words_from_all_docs=[]\n",
    "all_terms=[]\n",
    "foldr='essays/'\n",
    "fname='*.dat'\n",
    "\n",
    "for file_name in glob.glob('essays_word_per_file/*.dat'):\n",
    "    with open(file_name,'r') as fil:\n",
    "        words_in_file=fil.read().split(\"\\n\")\n",
    "    # remove empty strings from list of words\n",
    "    while \"\" in words_in_file:\n",
    "        words_in_file.remove(\"\")\n",
    "    # save the words per file as value in a dictionary\n",
    "    all_documents[file_name]=words_in_file\n",
    "    print(len(words_in_file),'words in',file_name)\n",
    "    # also save all the words to a list\n",
    "    for this_word in words_in_file:\n",
    "        all_words_from_all_docs.append(this_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_words_from_all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_from_all_docs = list(set(all_words_from_all_docs))\n",
    "len(all_words_from_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sizes are small, so results are not reliable representations of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_docs={}\n",
    "for doc_name, word_list_in_this_doc in all_documents.items():\n",
    "    if (len(word_list_in_this_doc)==0):\n",
    "        print(\"error: empty input file\"+doc_name)\n",
    "    else:\n",
    "        #print('\\n'+doc_name)\n",
    "        dic_of_terms={}\n",
    "        for this_term in word_list_in_this_doc:\n",
    "            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\n",
    "        #print(dic_of_terms)\n",
    "        dict_of_docs[doc_name]=dic_of_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since \"data\" appears in almost every document (these are data science essays), the word \"data\" is ranked low by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essays_word_per_file/week2 summary .docx.dat\n",
      "  top 5:\n",
      "('fix', 0.09772242758487387)\n",
      "('naming', 0.09772242758487387)\n",
      "('learning', 0.09772242758487387)\n",
      "('entry', 0.09772242758487387)\n",
      "('various', 0.09772242758487387)\n",
      "  bottom 5:\n",
      "('end', 0.08107678684747827)\n",
      "('type', 0.08107678684747827)\n",
      "('one', 0.07333700466920569)\n",
      "('data', 0.07189291103580682)\n",
      "('go', 0.0671935939402941)\n",
      "essays_word_per_file/week1_reading Summary.docx.dat\n",
      "  top 5:\n",
      "('constraint', 0.04574241291206862)\n",
      "('idea', 0.04574241291206862)\n",
      "('suggesting', 0.04574241291206862)\n",
      "('using', 0.04574241291206862)\n",
      "('perspective', 0.04574241291206862)\n",
      "  bottom 5:\n",
      "('ever', 0.03795083639669195)\n",
      "('line', 0.03507519733209504)\n",
      "('ten', 0.03507519733209504)\n",
      "('data', 0.03365200091037766)\n",
      "('act', 0.030159259881315285)\n",
      "essays_word_per_file/week1_Data 601- Summary of The History of Data Science .docx.dat\n",
      "  top 5:\n",
      "('exploring', 0.04606914443286911)\n",
      "('storing', 0.04606914443286911)\n",
      "('done', 0.04606914443286911)\n",
      "('using', 0.04606914443286911)\n",
      "('requiring', 0.04606914443286911)\n",
      "  bottom 5:\n",
      "('end', 0.03822191379952546)\n",
      "('high', 0.03822191379952546)\n",
      "('one', 0.034573159344054104)\n",
      "('data', 0.03389237234545179)\n",
      "('able', 0.03327086250981157)\n",
      "essays_word_per_file/week2_summary-Data Wrangling with Python  ch2 p17 to 40.docx.dat\n",
      "  top 5:\n",
      "('brackets', 0.08268820795643174)\n",
      "('description', 0.08268820795643174)\n",
      "('square', 0.08268820795643174)\n",
      "('underscores', 0.08268820795643174)\n",
      "('names', 0.08268820795643174)\n",
      "  bottom 5:\n",
      "('use', 0.0686034350247893)\n",
      "('key', 0.0686034350247893)\n",
      "('type', 0.0686034350247893)\n",
      "('put', 0.06340516440801795)\n",
      "('data', 0.06083246318414423)\n",
      "essays_word_per_file/week1_a History of Data Science.docx.dat\n",
      "  top 5:\n",
      "('names', 0.06449680220601675)\n",
      "('open', 0.06449680220601675)\n",
      "('statistics', 0.06449680220601675)\n",
      "('accept', 0.06449680220601675)\n",
      "('society', 0.06449680220601675)\n",
      "  bottom 5:\n",
      "('know', 0.055333894887275206)\n",
      "('thing', 0.055333894887275206)\n",
      "('try', 0.055333894887275206)\n",
      "('one', 0.04840242308167575)\n",
      "('data', 0.0474493212836325)\n",
      "essays_word_per_file/week1_Data Wrangling Chap 2.docx.dat\n",
      "  top 5:\n",
      "('understanding', 0.08062100275752095)\n",
      "('strings', 0.08062100275752095)\n",
      "('frequently', 0.08062100275752095)\n",
      "('review', 0.08062100275752095)\n",
      "('concepts', 0.08062100275752095)\n",
      "  bottom 5:\n",
      "('key', 0.06688834914916957)\n",
      "('low', 0.06329232324352231)\n",
      "('one', 0.06050302885209469)\n",
      "('data', 0.05931165160454063)\n",
      "('able', 0.058224009392170255)\n",
      "essays_word_per_file/week1_a Very Short History Of Data Science_1.docx.dat\n",
      "  top 5:\n",
      "('covered', 0.06718416896460078)\n",
      "('week', 0.06718416896460078)\n",
      "('expectations', 0.06718416896460078)\n",
      "('journals', 0.06718416896460078)\n",
      "('especially', 0.06718416896460078)\n",
      "  bottom 5:\n",
      "('present', 0.05996388583376802)\n",
      "('discipline', 0.05996388583376802)\n",
      "('format', 0.0541345547094407)\n",
      "('one', 0.0504191907100789)\n",
      "('data', 0.04942637633711719)\n",
      "essays_word_per_file/week1_ Summary.docx.dat\n",
      "  top 5:\n",
      "('using', 0.056084175831318916)\n",
      "('statistics', 0.056084175831318916)\n",
      "('hadoop', 0.056084175831318916)\n",
      "('service', 0.056084175831318916)\n",
      "('discussing', 0.056084175831318916)\n",
      "  bottom 5:\n",
      "('differ', 0.046531025495074485)\n",
      "('ten', 0.04300524194630784)\n",
      "('one', 0.04208906354928326)\n",
      "('data', 0.04126027937707174)\n",
      "('r', 0.0077975707450439816)\n",
      "essays_word_per_file/week1_summary.docx.dat\n",
      "  top 5:\n",
      "('towards', 0.04606914443286911)\n",
      "('books', 0.04606914443286911)\n",
      "('names', 0.04606914443286911)\n",
      "('practicing', 0.04606914443286911)\n",
      "('using', 0.04606914443286911)\n",
      "  bottom 5:\n",
      "('math', 0.03822191379952546)\n",
      "('name', 0.03822191379952546)\n",
      "('work', 0.03822191379952546)\n",
      "('ten', 0.03532573445589572)\n",
      "('data', 0.03389237234545179)\n",
      "essays_word_per_file/week1_1.txt.dat\n",
      "  top 5:\n",
      "('history', 0.13162612695105458)\n",
      "('chronologically', 0.13162612695105458)\n",
      "('peter', 0.13162612695105458)\n",
      "('conventional', 0.13162612695105458)\n",
      "('article', 0.13162612695105458)\n",
      "  bottom 5:\n",
      "('field', 0.12335132882639817)\n",
      "('specific', 0.12335132882639817)\n",
      "('require', 0.11292631609648)\n",
      "('state', 0.11292631609648)\n",
      "('data', 0.09683534955843368)\n",
      "essays_word_per_file/week2_Lists and Dictionaries Summary.docx.dat\n",
      "  top 5:\n",
      "('arrays', 0.054198993450434246)\n",
      "('operations', 0.054198993450434246)\n",
      "('means', 0.054198993450434246)\n",
      "('magnitude', 0.054198993450434246)\n",
      "('may', 0.054198993450434246)\n",
      "  bottom 5:\n",
      "('use', 0.04496695741120643)\n",
      "('key', 0.04496695741120643)\n",
      "('type', 0.04496695741120643)\n",
      "('one', 0.04067430511065189)\n",
      "('data', 0.03987337922994328)\n",
      "essays_word_per_file/week2_Data Wrangling with Python page 17 to 40.docx.dat\n",
      "  top 5:\n",
      "('turns', 0.04961292477385904)\n",
      "('append', 0.04961292477385904)\n",
      "('regular', 0.04961292477385904)\n",
      "('using', 0.04961292477385904)\n",
      "('pprint', 0.04961292477385904)\n",
      "  bottom 5:\n",
      "('type', 0.04116206101487358)\n",
      "('call', 0.03723263313975058)\n",
      "('data', 0.03649947791048654)\n",
      "('go', 0.03411367076968777)\n",
      "('us', 0.028529539974434418)\n",
      "essays_word_per_file/week2_Week 2 Reading Summary.docx.dat\n",
      "  top 5:\n",
      "('append', 0.07962568173582316)\n",
      "('strings', 0.07962568173582316)\n",
      "('support', 0.07962568173582316)\n",
      "('learning', 0.07962568173582316)\n",
      "('unordered', 0.07962568173582316)\n",
      "  bottom 5:\n",
      "('get', 0.06831345047811753)\n",
      "('key', 0.06606256706090821)\n",
      "('data', 0.058579408992138884)\n",
      "('eg', 0.05750519446140271)\n",
      "('pp', 0.053205111816116105)\n",
      "essays_word_per_file/Week2_summary.docx.dat\n",
      "  top 5:\n",
      "('troubleshooting', 0.059171378170657575)\n",
      "('highlighting', 0.059171378170657575)\n",
      "('using', 0.059171378170657575)\n",
      "('pprint', 0.059171378170657575)\n",
      "('tour', 0.059171378170657575)\n",
      "  bottom 5:\n",
      "('data', 0.04353148741617661)\n",
      "('inc', 0.04199888113486316)\n",
      "('go', 0.040686029358343215)\n",
      "('pp', 0.03953774364316885)\n",
      "('us', 0.03402605685024288)\n",
      "essays_word_per_file/week2_Summary Week 2.docx.dat\n",
      "  top 5:\n",
      "('append', 0.0551254719709545)\n",
      "('multiply', 0.0551254719709545)\n",
      "('using', 0.0551254719709545)\n",
      "('variables', 0.0551254719709545)\n",
      "('module', 0.0551254719709545)\n",
      "  bottom 5:\n",
      "('way', 0.04573562334985954)\n",
      "('key', 0.04573562334985954)\n",
      "('type', 0.04573562334985954)\n",
      "('data', 0.04055497545609616)\n",
      "('int', 0.03588366172645454)\n",
      "essays_word_per_file/week1_summary-50 years of data science.docx.dat\n",
      "  top 5:\n",
      "('promote', 0.09772242758487387)\n",
      "('people', 0.09772242758487387)\n",
      "('willing', 0.09772242758487387)\n",
      "('disputed', 0.09772242758487387)\n",
      "('adapt', 0.09772242758487387)\n",
      "  bottom 5:\n",
      "('skill', 0.08722019757638985)\n",
      "('invest', 0.08383923467768971)\n",
      "('program', 0.08383923467768971)\n",
      "('part', 0.08107678684747827)\n",
      "('data', 0.07189291103580682)\n",
      "essays_word_per_file/week2_Data601-Reading Assignment_2.docx.dat\n",
      "  top 5:\n",
      "('append', 0.054198993450434246)\n",
      "('names', 0.054198993450434246)\n",
      "('using', 0.054198993450434246)\n",
      "('formatting', 0.054198993450434246)\n",
      "('operations', 0.054198993450434246)\n",
      "  bottom 5:\n",
      "('way', 0.04496695741120643)\n",
      "('use', 0.04496695741120643)\n",
      "('type', 0.04496695741120643)\n",
      "('set', 0.043671573547111826)\n",
      "('data', 0.03987337922994328)\n",
      "essays_word_per_file/week1_assignment 1 Summary.pdf.dat\n",
      "  top 5:\n",
      "('j', 0.23324380122195446)\n",
      "('q', 0.2202651983785375)\n",
      "('z', 0.2202651983785375)\n",
      "('x', 0.1865909870690755)\n",
      "('k', 0.16577468432233047)\n",
      "  bottom 5:\n",
      "('c', 0.06951017718532497)\n",
      "('l', 0.06608800371521313)\n",
      "('r', 0.04981781309333655)\n",
      "('n', 0.04547046882112831)\n",
      "('e', 0.024084614494628988)\n",
      "essays_word_per_file/week2_Summary on Chapter 2 of Data Wrangling in Python.docx.dat\n",
      "  top 5:\n",
      "('towards', 0.061425525910492156)\n",
      "('troubleshooting', 0.061425525910492156)\n",
      "('using', 0.061425525910492156)\n",
      "('equipped', 0.061425525910492156)\n",
      "('sympy', 0.061425525910492156)\n",
      "  bottom 5:\n",
      "('type', 0.05096255173270063)\n",
      "('set', 0.049494450020060074)\n",
      "('one', 0.04609754579207215)\n",
      "('data', 0.045189829793935724)\n",
      "('go', 0.04223597333389915)\n",
      "essays_word_per_file/week2_Week 2 - Summary of DATA WRANGLING WITH PYTHON Chapter 2.docx.dat\n",
      "  top 5:\n",
      "('kinds', 0.05286623131640718)\n",
      "('covered', 0.05286623131640718)\n",
      "('qualities', 0.05286623131640718)\n",
      "('variables', 0.05286623131640718)\n",
      "('modify', 0.05286623131640718)\n",
      "  bottom 5:\n",
      "('set', 0.042597682394313996)\n",
      "('hand', 0.042597682394313996)\n",
      "('data', 0.03889288629805943)\n",
      "('act', 0.03485619379725784)\n",
      "('us', 0.030400329480954707)\n",
      "essays_word_per_file/week1_380.txt.dat\n",
      "  top 5:\n",
      "('covered', 0.06084603981699693)\n",
      "('using', 0.06084603981699693)\n",
      "('statistics', 0.06084603981699693)\n",
      "('cases', 0.06084603981699693)\n",
      "('two', 0.06084603981699693)\n",
      "  bottom 5:\n",
      "('require', 0.052201787629504906)\n",
      "('use', 0.05048177294276948)\n",
      "('set', 0.04902752124628592)\n",
      "('ten', 0.04665663041344718)\n",
      "('data', 0.04476351064493632)\n",
      "essays_word_per_file/week1_Assignment1.docx.dat\n",
      "  top 5:\n",
      "('report', 0.0444805532455288)\n",
      "('peter', 0.0444805532455288)\n",
      "('idea', 0.0444805532455288)\n",
      "('using', 0.0444805532455288)\n",
      "('focus', 0.0444805532455288)\n",
      "  bottom 5:\n",
      "('use', 0.036903916771955626)\n",
      "('math', 0.036903916771955626)\n",
      "('rate', 0.036903916771955626)\n",
      "('data', 0.03272366985078103)\n",
      "('w', 0.019214610858427787)\n",
      "essays_word_per_file/week1_A Very Short History Of Data Science.docx.dat\n",
      "  top 5:\n",
      "('little', 0.06861361936810292)\n",
      "('excited', 0.06861361936810292)\n",
      "('realized', 0.06861361936810292)\n",
      "('breed', 0.06861361936810292)\n",
      "('statistics', 0.06861361936810292)\n",
      "  bottom 5:\n",
      "('thing', 0.058865845624760854)\n",
      "('help', 0.058865845624760854)\n",
      "('math', 0.05692625459503793)\n",
      "('one', 0.05149193944859122)\n",
      "('data', 0.05047800136556649)\n",
      "essays_word_per_file/week1_50 Years Data Science Summary.docx.dat\n",
      "  top 5:\n",
      "('using', 0.04849383624512538)\n",
      "('statistics', 0.04849383624512538)\n",
      "('searching', 0.04849383624512538)\n",
      "('puzzled', 0.04849383624512538)\n",
      "('among', 0.04849383624512538)\n",
      "  bottom 5:\n",
      "('work', 0.0402335934731847)\n",
      "('part', 0.0402335934731847)\n",
      "('one', 0.036392799309530634)\n",
      "('data', 0.03567618141626504)\n",
      "('um', 0.033344189474130904)\n"
     ]
    }
   ],
   "source": [
    "for doc_name, dic_of_terms in dict_of_docs.items():\n",
    "    print(doc_name)\n",
    "    terms_in_doc_sorted_by_score = sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('  top 5:')\n",
    "    for word_and_score in terms_in_doc_sorted_by_score[0:5]:\n",
    "        print(word_and_score)\n",
    "    print('  bottom 5:')\n",
    "    for word_and_score in terms_in_doc_sorted_by_score[-5:]:\n",
    "        print(word_and_score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#        =sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        # first 40 words by importance\n",
    "        for this_tup in terms_in_doc_sorted_by_score[0:40]:\n",
    "            print(this_tup)\n",
    "#        for indx in range(10):\n",
    "#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
