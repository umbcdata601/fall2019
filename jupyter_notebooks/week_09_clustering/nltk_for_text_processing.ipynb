{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Natural_Language_Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a one-time action to get a list of stopwords for NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bradezone.com/2008/09/13/boring/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_text=\"I go to the store. A car is parked. \\\n",
    "Many cars are parked or moving. Some are blue. \\\n",
    "Some are tan. They have windows. In the store, \\\n",
    "there are items for sale. These include such \\\n",
    "things as soap, detergent, magazines, and lettuce. \\\n",
    "You can enhance your life with these products. \\\n",
    "Soap can be used for bathing, be it in a bathtub \\\n",
    "or in a shower. My email address is myname@sc.edu. \\\n",
    "Apply the soap to your body and rinse. My phone \\\n",
    "number is 452-953-2942. Detergent is used to \\\n",
    "wash clothes. Place your dirty clothes \\\n",
    "into a washing machine and add some detergent \\\n",
    "as directed on the box. Your email is \\\n",
    "aperson@farm.com and your cell is 595-942-2424. \\\n",
    "Select the appropriate settings on your \\\n",
    "washing machine and you should be ready to \\\n",
    "begin. Magazines are stapled reading material \\\n",
    "made with glossy paper, and they cover a wide \\\n",
    "variety of topics, ranging from news and \\\n",
    "politics to business and stock market information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I go to the store.',\n",
       " 'A car is parked.',\n",
       " 'Many cars are parked or moving.',\n",
       " 'Some are blue.',\n",
       " 'Some are tan.',\n",
       " 'They have windows.',\n",
       " 'In the store, there are items for sale.',\n",
       " 'These include such things as soap, detergent, magazines, and lettuce.',\n",
       " 'You can enhance your life with these products.',\n",
       " 'Soap can be used for bathing, be it in a bathtub or in a shower.',\n",
       " 'My email address is myname@sc.edu.',\n",
       " 'Apply the soap to your body and rinse.',\n",
       " 'My phone number is 452-953-2942.',\n",
       " 'Detergent is used to wash clothes.',\n",
       " 'Place your dirty clothes into a washing machine and add some detergent as directed on the box.',\n",
       " 'Your email is aperson@farm.com and your cell is 595-942-2424.',\n",
       " 'Select the appropriate settings on your washing machine and you should be ready to begin.',\n",
       " 'Magazines are stapled reading material made with glossy paper, and they cover a wide variety of topics, ranging from news and politics to business and stock market information.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize_list = sent_tokenize(this_text)\n",
    "sent_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'go', 'to', 'the', 'store', '.']\n",
      "['A', 'car', 'is', 'parked', '.']\n",
      "['Many', 'cars', 'are', 'parked', 'or', 'moving', '.']\n",
      "['Some', 'are', 'blue', '.']\n",
      "['Some', 'are', 'tan', '.']\n",
      "['They', 'have', 'windows', '.']\n",
      "['In', 'the', 'store', ',', 'there', 'are', 'items', 'for', 'sale', '.']\n",
      "['These', 'include', 'such', 'things', 'as', 'soap', ',', 'detergent', ',', 'magazines', ',', 'and', 'lettuce', '.']\n",
      "['You', 'can', 'enhance', 'your', 'life', 'with', 'these', 'products', '.']\n",
      "['Soap', 'can', 'be', 'used', 'for', 'bathing', ',', 'be', 'it', 'in', 'a', 'bathtub', 'or', 'in', 'a', 'shower', '.']\n",
      "['My', 'email', 'address', 'is', 'myname', '@', 'sc.edu', '.']\n",
      "['Apply', 'the', 'soap', 'to', 'your', 'body', 'and', 'rinse', '.']\n",
      "['My', 'phone', 'number', 'is', '452-953-2942', '.']\n",
      "['Detergent', 'is', 'used', 'to', 'wash', 'clothes', '.']\n",
      "['Place', 'your', 'dirty', 'clothes', 'into', 'a', 'washing', 'machine', 'and', 'add', 'some', 'detergent', 'as', 'directed', 'on', 'the', 'box', '.']\n",
      "['Your', 'email', 'is', 'aperson', '@', 'farm.com', 'and', 'your', 'cell', 'is', '595-942-2424', '.']\n",
      "['Select', 'the', 'appropriate', 'settings', 'on', 'your', 'washing', 'machine', 'and', 'you', 'should', 'be', 'ready', 'to', 'begin', '.']\n",
      "['Magazines', 'are', 'stapled', 'reading', 'material', 'made', 'with', 'glossy', 'paper', ',', 'and', 'they', 'cover', 'a', 'wide', 'variety', 'of', 'topics', ',', 'ranging', 'from', 'news', 'and', 'politics', 'to', 'business', 'and', 'stock', 'market', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "for this_sent in sent_tokenize_list:\n",
    "    word_tokens = word_tokenize(this_sent) \n",
    "    print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'go', 'store', '.']\n",
      "['A', 'car', 'parked', '.']\n",
      "['Many', 'cars', 'parked', 'moving', '.']\n",
      "['Some', 'blue', '.']\n",
      "['Some', 'tan', '.']\n",
      "['They', 'windows', '.']\n",
      "['In', 'store', ',', 'items', 'sale', '.']\n",
      "['These', 'include', 'things', 'soap', ',', 'detergent', ',', 'magazines', ',', 'lettuce', '.']\n",
      "['You', 'enhance', 'life', 'products', '.']\n",
      "['Soap', 'used', 'bathing', ',', 'bathtub', 'shower', '.']\n",
      "['My', 'email', 'address', 'myname', '@', 'sc.edu', '.']\n",
      "['Apply', 'soap', 'body', 'rinse', '.']\n",
      "['My', 'phone', 'number', '452-953-2942', '.']\n",
      "['Detergent', 'used', 'wash', 'clothes', '.']\n",
      "['Place', 'dirty', 'clothes', 'washing', 'machine', 'add', 'detergent', 'directed', 'box', '.']\n",
      "['Your', 'email', 'aperson', '@', 'farm.com', 'cell', '595-942-2424', '.']\n",
      "['Select', 'appropriate', 'settings', 'washing', 'machine', 'ready', 'begin', '.']\n",
      "['Magazines', 'stapled', 'reading', 'material', 'made', 'glossy', 'paper', ',', 'cover', 'wide', 'variety', 'topics', ',', 'ranging', 'news', 'politics', 'business', 'stock', 'market', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "for this_sent in sent_tokenize_list:\n",
    "    filtered_sentence = [] \n",
    "    word_tokens = word_tokenize(this_sent) \n",
    "    for w in word_tokens: \n",
    "        if w not in en_stops: \n",
    "            filtered_sentence.append(w) \n",
    "    print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
